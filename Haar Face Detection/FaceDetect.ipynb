{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Lab_Submission1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt8HaPEhEOMr"
      },
      "source": [
        "Author: Rui ZHANG \n",
        "\n",
        "Department: EIT\n",
        "\n",
        "E-mail: cheryl_zhangrui@163.com / rui.zhang1@etu.unice.fr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSNIrI3dEYDu"
      },
      "source": [
        "# **0 Readme**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEN9i1tkEf3o"
      },
      "source": [
        "This notebook gives the example of using the laptop camera and opencv-python Haar Feature-based cascade face detector of Adaboost classifier to detect human face. The notebook is structured as follows:\n",
        "\n",
        "1. Import Libraries\n",
        "2. Helper Functions\n",
        "3. Face Detection (Simple)\n",
        "4. Face Detection (Accelerated)\n",
        "\n",
        "The explanation of the method, steps and result analysis will follow the title of that part.\n",
        "\n",
        "To run the code, please run the cells in sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BYhNrtsapw0"
      },
      "source": [
        "# **1 Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taluhcwMa0YE"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St3z0IFZa2Y0"
      },
      "source": [
        "# **2 Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQf9yfN0bs72"
      },
      "source": [
        "### **2.1 Camera calling and display**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D82H60XkbMGE"
      },
      "source": [
        "Javascript code to call the camera and display the video and image in Google Coloab (from teacher)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS1b6Nj--uPE"
      },
      "source": [
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61nDL0SUlsTR"
      },
      "source": [
        "### **2.2 Other helper functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qAoLI0Zo-re"
      },
      "source": [
        "Includes the following functions:\n",
        "\n",
        "**clock()**: timing function\n",
        "\n",
        "**draw_rects(img, rect, color)**: function to draw rectangles in the image, to show the detection result\n",
        "\n",
        "**draw_string(dst, target, s)**: function to put a string in the image, to show the detection time\n",
        "\n",
        "**detect(img, cascade)**: use the classifier to detect face and return the result\n",
        "\n",
        "**expand_rect(rect, pix)**: expand a given rect with certain pixels to be the detection ROI (region of interest)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU6uSvu3mYWH"
      },
      "source": [
        "def clock():\n",
        "  \"\"\"\n",
        "  Get the time (calculated by clock cycles / clock frequency).\n",
        "  From opencv examples common.py.\n",
        "  :return: Present timestamp (in seconds).\n",
        "  \"\"\"\n",
        "  return cv2.getTickCount() / cv2.getTickFrequency()\n",
        "\n",
        "\n",
        "def draw_rects(img, rects, color):\n",
        "  \"\"\"\n",
        "  Draw rectangles in a given image using the given rectangle list.\n",
        "  From opencv examples facedetect.py.\n",
        "  :param img: the image on which to be drawn the rectangles\n",
        "  :param rect: a list of rectangles' coordinates, in the order [x1, y1, x2, y2]\n",
        "  :param color: draw color\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  for rect in rects:\n",
        "    [x1, y1, x2, y2] = rect\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "\n",
        "def draw_str(dst, target, s):\n",
        "  \"\"\"\n",
        "  Put a string of text on the given image.\n",
        "  From opencv examples common.py.\n",
        "  :param dst: string position\n",
        "  :param target: the image to be put text on\n",
        "  :param s: the string to be shown\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  x, y = target\n",
        "  # Strings with 1 pixel translation in x and y and different colors to make the text more obvious to be seen\n",
        "  cv2.putText(dst, s, (x+1, y+1), cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 0), thickness = 2, lineType=cv2.LINE_AA)\n",
        "  cv2.putText(dst, s, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.0, (255, 255, 255), lineType=cv2.LINE_AA)\n",
        "\n",
        "\n",
        "def detect(img, cascade):\n",
        "  \"\"\"\n",
        "  Detect the face in the input image using the cascade detector.\n",
        "  From opencv examples facedetect.py.\n",
        "  :param img: the input image to be detected\n",
        "  :param cascade: the classifier\n",
        "  :return: the detected faces, in a list of rectangles\n",
        "  \"\"\"\n",
        "  # Detect faces of different size in the input image, reduce the image size by 30% when constructing the image \n",
        "  # pyramid, set the minimum face to be detected at size 30x30\n",
        "  rects = cascade.detectMultiScale(img, scaleFactor=1.3, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "  # No face detected in the input image\n",
        "  if len(rects) == 0:\n",
        "    return []\n",
        "  # The detected result is given as [x, y, w, h], change it to [x1, y1, x2, y2]=[x, y, x+w, y+h] for convenience\n",
        "  rects[:,2:] += rects[:,:2]\n",
        "  return rects\n",
        "\n",
        "\n",
        "def expand_rect(rect, pix):\n",
        "  \"\"\"\n",
        "  Expand the given rectangle by expanding outwards 'pix' pixels at each side\n",
        "  :param rect: the coordinates of the original rectangle [x1, y1, x2, y2]\n",
        "  :param pix: the pixels to be expanded at each side\n",
        "  :return: the coordinates of the expanded rectangle [x1', y1', x2', y2']\n",
        "  \"\"\"\n",
        "  expanded = [rect[0]-pix, rect[1]-pix, rect[2]+pix, rect[3]+pix]\n",
        "  return [int(item) for item in expanded]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4zyKkfk4ngk"
      },
      "source": [
        "# **3 Face Detection (Simple)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8xSXPspqQ2s"
      },
      "source": [
        "**Steps:**\n",
        "\n",
        "The process of the face detection in the whole image: \n",
        "\n",
        "1. create a real-time video stream\n",
        "2. load the cascade classifier\n",
        "3. capture the present frame and preprocessing (format and color transformation)\n",
        "4. detect the face in the image\n",
        "5. show the result and detection time, go to step 3 until stop.\n",
        "\n",
        "Detailed explanations of each step are within the code.\n",
        "\n",
        "**Result:**\n",
        "\n",
        "This simple version of face detection time ranges from 50~80ms per detection depending on the background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1C2qvRkmmjx",
        "outputId": "dc9c5fac-0671-45a3-b619-ab14ef1b626b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "# Create a real-time video object using the javascript code.\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "# Load the trained Haar Feature-based cascade classifier from the xml file, which contains the features of the\n",
        "# 38 classifiers, and their corresponding weights\n",
        "cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\")\n",
        "\n",
        "while True:\n",
        "  # Capture the present image\n",
        "  byte = eval_js('capture()')\n",
        "\n",
        "  # Format transformation, transform the image to gray\n",
        "  im = byte2image(byte)\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Start time\n",
        "  t = clock()\n",
        "\n",
        "  # Detection\n",
        "  rects = detect(gray, cascade)\n",
        "\n",
        "  # End time\n",
        "  dt = clock() - t\n",
        "\n",
        "  # Draw the detected rectangles, put the detection time (ms), show the image\n",
        "  draw_rects(im, rects, (255, 0, 0))\n",
        "  draw_str(im, (20, 20), 'time: %.1f ms' % (dt * 1000))\n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(im)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-be13acc19762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mdraw_rects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mdraw_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time: %.1f ms'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showimg(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: Cell has no view"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ__vkVX41Ix"
      },
      "source": [
        "# **4 Face Detection (Accelerated)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzI3K0mMq-Cm"
      },
      "source": [
        "**Steps:**\n",
        "\n",
        "The accelerated version of face detection:\n",
        "\n",
        "1. create a real-time video stream\n",
        "2. load the cascade classifier\n",
        "3. capture an image and preprocessing (format and color transformation)\n",
        "4. if RESTART==True, restart detecting a face in the whole image; if RESTART=False, detect a face in the ROI (region of interest, which is an expanded area of the former detection result)\n",
        "5. detect the face\n",
        "6. update the ROI and RESTART according to the detection result\n",
        "7. show the result and detection time, go to step 3 until stop.\n",
        "\n",
        "Detailed explanations of each step are within the code.\n",
        "\n",
        "**Result:**\n",
        "\n",
        "The accelerated version achives around half of the detecting time than before (from 60ms to 30ms on my laptop, may change according to the background, the network, the computer camera, etc.) when detecting a face in the ROI (this can be speeded up by decreasing the pixels to be expanded around the ROI, but when expand less, the possibility to detect a fast-moving face in the ROI also decrease, thus leads to more restarts), and the same detection time when restart detection using the whole image as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpXvKC0omxNd",
        "outputId": "ee0318b8-a3c4-47b3-d4d3-c4820e34cb13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "# Create a real-time video object using the javascript code.\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "# Reset the RESTART flag to be True, which means a face is not detected in the former frame or the start of the\n",
        "# detection, we need to restart to detect the face using the whole image\n",
        "RESTART = True\n",
        "\n",
        "# Load the trained Haar Feature-based cascade classifier from the xml file, which contains the features of the\n",
        "# 38 classifiers, and their corresponding weights\n",
        "cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\")\n",
        "\n",
        "while True:\n",
        "    # Capture the present frame, transform its format and to gray image\n",
        "    byte = eval_js('capture()')\n",
        "    im = byte2image(byte)\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Clock start\n",
        "    t = clock()\n",
        "\n",
        "    ## If RESTART==True, detect in the whole image\n",
        "    if RESTART:\n",
        "        det_rect = detect(gray, cascade)\n",
        "        ## If no face is detected, reset RESTART=True, continue to next round detection\n",
        "        if len(det_rect) == 0:\n",
        "            RESTART = True\n",
        "            continue\n",
        "\n",
        "        ## If a face is detected:\n",
        "        # 1) set RESTART=False, which means next round we don't have to detect in the whole image\n",
        "        # 2) select the first detected rectangle, expand it to be the roi to be detected in the next round\n",
        "        # 3) draw the rectangle\n",
        "        else:\n",
        "            dt = clock() - t\n",
        "            RESTART = False\n",
        "            rect = det_rect[0]\n",
        "            roi = expand_rect(rect, 50)\n",
        "            draw_rects(im, [rect], (255, 0, 0))\n",
        "\n",
        "    ## If RESTART=False, detect in the roi\n",
        "    else:\n",
        "        det_rect = detect(gray[roi[1]:roi[3], roi[0]:roi[2]], cascade)\n",
        "        ## If no face is detected, reset RESTART, continue to next round detection\n",
        "        if len(det_rect) == 0:\n",
        "            RESTART = True\n",
        "            continue\n",
        "\n",
        "        ## If a face is detected:\n",
        "        # 1) select the first detected ranctangle to move on\n",
        "        # 2) map the coordinates in the roi to the original image by adding the top left point's coordinates of\n",
        "        #   the roi to each coordinates of the detected result (as the detection result is within the roi)\n",
        "        # 3) draw the mapped rectangle in original image\n",
        "        # 4) expand the rectangle to be the roi in next round\n",
        "        else:\n",
        "            dt = clock() - t\n",
        "            rect = det_rect[0]\n",
        "            mapped = [rect[0]+roi[0], rect[1]+roi[1], rect[2]+roi[0], rect[3]+roi[1]]\n",
        "            draw_rects(im, [mapped], (255, 0, 0))\n",
        "            roi = expand_rect(mapped, 50)\n",
        "\n",
        "    # Put the string of elapsed time (ms), show the image\n",
        "    draw_str(im, (20, 20), \"time: %.1f ms\" % (dt * 1000))\n",
        "    eval_js('showimg(\"{}\")'.format(image2byte(im)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-93bf1bc701e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Capture the present frame, transform its format and to gray image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: Cell has no view"
          ]
        }
      ]
    }
  ]
}